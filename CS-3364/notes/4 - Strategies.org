
* Strategies
** Divide & Conqurer
Given of problem of size $n$.
Suppose we have $k$ subproblems.

(essentially $k \subseteq n$)

Suppose we have a set of solutions to these problems where $s_1$ is a solution to $p_1$, $s_2$ is a solution to $p_2$, $\cdots$, $s_k$ is a solution to $p_k$.
- Should be solving the same problem that the original problem is solving.
- $s_k$ should be able to obtain solutions for $p_k$.
- We should be able to write a method to combine all of the solutions.

Examples:
- Binary Search.
- Merge Sort.
- Finding Max and Min elements.
- Quick Sort.
- Strassen's Matrix Multipication.


#+begin_src C
#include <stdio.h>
void Test(int n) {
    if(n > 0){
        printf("%d\n", n);
        Test(n - 1);
    }
}

Test(3);
#+end_src

#+RESULTS:
| 3 |
| 2 |
| 1 |

How many times is it printing? $n$ times.

** Factorial example
$n!=1\times2\times3\times\cdots\times(n-1)\times n$

$n!= n\times (n-1)!$

Rule: $0! := 1$

Let's say there's an algorithm $F(n)$:
#+BEGIN_EXAMPLE
Algo F(n):
        if(n==0):
                return 1
        else
                return n * F(n-1)
#+END_EXAMPLE

In C:
#+begin_src C
#include <stdio.h>
int numOfMult;
int F(int n){
    numOfMult++;
    if(n == 0){
        return(1);
    } else {
        return(n * F(n - 1));
    }
}

int factorial = F(3);
printf("Total: %d, Number of Multipications: %d", factorial, numOfMult);
#+end_src

#+RESULTS:
| Total: 6 | Number of Multipications: 4 |

$T(n)=1+T(n-1)$

$T(3)= 1 + 1 + 1 + 1$, where $F(3) = 3 \times 2 \times 1 \times 1$.


** Repeated Substutition
Example:
- $T(0)=0$
- $T(n)=1+T(n-1)$
- $T(n)=2+T(n-2)$
- $T(n)=3+T(n-3)$
- $\vdots$
- $T(n)=K+T(n-K)$

Overall, the equation gained from this is:
- $T(n)=K+T(n-K)$ where $T(0)=0$
- $T(n)=?+T(k)$
- How large does $n-K$ have to have to get to $0$?
- $n-K=0\implies n=K$
- You have to recurse $n$ times to get to the base case.
- $\therefore T(n)=n$ s.t. $\theta(n)$

** Algorithms
1. Identify the Input Size
2. Identify the Basic Operation
3. Setup a recurrence relation that expresses how many time the basic operation executes as a function of $n$.
4. Solve or at least determine the order of growth. Find $\theta$.

Professors way:
$T(n)=\text{(some function)}\times T(\text{(some function of n)}+\text{(some function of n)})$
$T(n_0)=\text{(something)}$

1. Rewrite the relation. Replace $n$ with $x$.
   $T(n)=\cdots$
2. Using $T(x)$, workout a new expression to replace the reoccurence part $T(?)$
3. Plug new expression back into the original recurrence relation.
4. Repeat step 2 and step 3 several more times until you see a pattern.

Pattern:
- $T(n)=F(n,k)+T(G(n,k)+H(n,k))$ where $F(n,k)$ is some function of $n,k$

*** Example
Connect the pattern back to the initial condition.

Anchor: Connect the pattern expression to the base case
$T(G(n,k))=T(n_0)$ where
- $T(n_0)$ is the amount of time taken to complete the base case.
- Solve $T(G(n,k))$ to obtain what $k$ is.

Plug $K(\text{value})$ back into the pattern expression from step 4.

**** Ex. 1
- $T(n)=T(\frac{n}{2})$
- $T(1)=c$ where $c$ is some number $>0$.
- Substitute
- $T(x)=T(\frac{x}{2})$
- $T(\frac{n}{2})=T(\frac{\frac{n}{2}}{2})=T(\frac{n}{4})$
- $\vdots$
- $\implies T(n)=T(\frac{n}{8})=T(\frac{n}{2^3})$
- $\implies T(n)=T(\frac{n}{2^k})$
- $T(1)=c$
- $T(\frac{n}{2^k})=T(1)$
- $\implies \frac{n}{2^k}=1$
- $\implies n=2^k$
- $\implies k=\log(n)$
- $T(n)=T(\frac{n}{2^{\log_2(n)}})\implies T(n)=T(1)$
- $\implies T(n)=T(1)=c(\text{some constant more than 0.})$

- $\therefore T(n)=\theta(1)$

**** Ex. 2
- $T(n)=T(\frac{n}{2})+c_1$
- $T(1)=c_2$
- $T(n)=T(\frac{n}{2})+c_1$
- $\implies T(x)=T(\frac{x}{2})+c_1$
- $\implies T(\frac{n}{2})=T(\frac{n}{4})+c_1$
- $\implies T(n)=T(\frac{n}{4})+c_1+c_1$
- $\implies T(\frac{n}{4})=T(\frac{n}{8})+c_1$
- $\implies T(n)=T(\frac{n}{8})+c_1+c_1+c_1$

So now we have a pattern.
- $T(n)=T(\frac{n}{2^k})+k\times c_1$
- $\implies T(n): T(\frac{n}{n})+\log(n)\times c_1$
- $\implies T(1)+c_1\log(n)$
- $\implies T(n)=T(1)+c_1\log(n)$
- $\therefore T(n)=c_1\log(n)+c_2$

**** Ex. 3
- $T(n)=2\times T(\frac{n}{2})+n$ where $T(1)=c$

Iteration 1:
- $\implies T(x)=2T(\frac{x}{4})+n$
- $\implies 2T(\frac{x}{2})=4T(\frac{x}{4})+2x$
- $\implies T(n)=4T(\frac{n}{4})+2n+n$
- $\implies T(n)=4T(\frac{n}{4})+3n$

Iteration 2:
- $\implies T(n)=4T(\frac{n}{4})+2n+n$
- $\implies 4T(\frac{n}{4})=8T(\frac{n}{8})+4n$
- $\implies T(n)=8T(\frac{n}{8})+4n+2n+n$
- $\implies T(n)=8T(\frac{n}{8})+7n$

Pattern:
- $\implies T(n)=2^kT(\frac{n}{2^k})+(2^{k+1}-1)n$

WRONG!!! HAHA loser. Real answer:
- $\implies T(n)=2^kT(\frac{n}{2^k})+kn$ where $k=\log(n)$
- $\implies nT(\frac{n}{n})+n\log(n)$
- $\therefore T(n)=nc+n\log(n)$

**** Ex. 4: "Tower of Hanoii"
- $T(n)=2\times T(n-1)+1$
- $T(1)=1$

Iteration 1:
- $\implies T(x)=2T(x-1)+1$
- $\implies T(x-1)=4T(x-1-1)+1+1$
- $\implies T(n)=4T(n-2)+2+1$

Iteration 2:
- $\implies T(x)=4T(x-2)+3$
- $\implies T(x-1)=8T(x-2-1)+3+1$
- $\implies T(n)=8T(x-3)+3$

My educated guess:
- $\implies T(n)=2^{k}T(x-k)+(k-1)$ where $k=\log(n)$
- $\implies T(n)=2^{\log(n)}T(x-\log(n))+\log(n)-1$
- $\therefore T(n)=nT(x-\log(n))+\log(n)-1$

WRONG!!! yet again! haha loser core. loser. WHY!?:
- you aren't multiplying. you NEED to. it's ITERATIVE
- $2f(x+1)\implies 2f(4f(x+2)+1)$
- whereas you're doing $2f(x+1)\implies 2*4f(x+2)\cdots$

Answer:
- $T(n)=2^{n-1}T(1)+2^n-1$ where $\theta(2^n)$

*** Case 1
Overhead is dominant.
- $f(n)\in \omega(n^c)$ which is overhead dominating. ($T(n)\in\theta(f(n))$)
- What if $a$ is smaller relative to $f(n)$?
- $T(n)=2T(\frac{n}{2})+n^3$ where $a=2,b=2,f(n)=n^3$
- If $f(n)\in \Omega (n^d), d>c \equiv d>\log_b a \equiv  b^d > a$, then $T(n)=\theta(f(n))$

Process:
- $\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot \implies 8^3=512$
- $(\cdot\cdot\cdot\cdot)(\cdot\cdot\cdot\cdot) \implies 2(\frac{n}{2})^3\implies 2(\frac{8}{2})^3\implies 128$
- $(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)\implies 4(\frac{\frac{n}{2}}{2})^3\implies 32$
- $(\cdot)(\cdot)(\cdot)(\cdot)(\cdot)(\cdot)(\cdot)(\cdot)\implies 8(\frac{\frac{\frac{n}{2}}{2}}{2})^3\implies 8$
- $n:$ linear number of recursions
- $n^3:$ overhead


*** Case 2
Recusrion is domanint.
- $f(n)\in O(n^c)$ which is recursion domanating. ($T(n)\in\theta(n^c)$)

If recursion is donminating relative to overhead, what if $f(n)$ is small?
- $T(n)=3T(\frac{n}{2})+1$
- $\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot \implies 3^0 \implies 1$. This is 8 recursive calls.
- $(\cdot\cdot\cdot\cdot)(\cdot\cdot\cdot\cdot)(\cdot\cdot\cdot\cdot)\implies 3^1\implies3$. 3 4 element recursive calls.
- $(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)(\cdot\cdot)\implies 3^2\implies 9$. 9 2 element recursive calls.
- $(\cdot)\times 27 \implies 3^3 \implies 27$. 27 one element recursive calls.
- There is an exponential increase of recursive calls as $n$ increases.
- $\therefore T(n)\in \theta (n^{\log_2(3)})\approxeq \theta(n^{1.58})$.

*** Case 3
Balanced case.
- $T(n)=2T(\frac{n}{2})+n$ which is balanced. ($T(n)=\theta(n^c\log_b(n))$)
- $T(n)=\theta(n\log(n))$
- If $f(n)\in \theta(n^c(\log n)^{k+1}), k\geq 0$, then $T(n) = \theta(n^c(\log n)^{k+1})$
- In other words, if $f(n)$ is tuned exactly right, then overall reoccurance is same overhead with merely a log slow down.

Master theorem:
- Let $T(n)=aT(\frac{n}{b})+f(n)$ where $a \geq 1, b > 1, c=\log_b(a)$
- Critical component, where threshold is between the overhead and recursion dominating.


* Strassens matrix multipication theorem
- $P_1=A_{11}\times (B_{12} - B_{22})$
- $P_2=(A_{11}+A_{12})\times B_{22}$
- $P_3=(A_{21}+A_{22})\times B_{11}$
- $P_4=A_{22}\times (B_{21}-B_{11})$
- $P_5=(A_{12}+A_{22})\times(B_{11}+B_{22})$
- $P_6=(A_{12}-A_{22})\times(B_{21}+B_{22})$
- $P_7=(A_{11}-A_{21})\times(B_{11}+B_{12})$

- $C_{11}=$
- $C_{12}=$
- $C_{21}=$
- $C_{22}=$
